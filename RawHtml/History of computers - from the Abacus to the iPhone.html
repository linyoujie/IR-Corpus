<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">
<html>
<head>
    <link rel="canonical" href="https://www.explainthatstuff.com/historyofcomputers.html">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta http-equiv="Content-type" content="text/html;charset=UTF-8">
  <title>History of computers - from the Abacus to the iPhone</title>
  <meta name="description" content="An easy-to-understand history of computers, from the abacus to the Internet and iPhone.">
<link rel="shortcut icon" href="https://cdn4.explainthatstuff.com/favicon.ico">
<meta property="og:url" content="http://www.explainthatstuff.com/historyofcomputers.html">
<meta property="og:type" content="article">
<meta property="og:site_name" content="Explain that Stuff">
<meta property="og:title" content="History of computers - from the Abacus to the iPhone">
<meta property="og:description" content="An easy-to-understand history of computers, from the abacus to the Internet and iPhone">
<meta property="og:image" content="https://cdn4.explainthatstuff.com/herman-hollerith-statistics-patent.png">
<meta property="fb:app_id" content="1678539732394615">

<link rel="dns-prefetch" href="https://cdn4.explainthatstuff.com/">
<link rel="prefetch" href="https://cdn4.explainthatstuff.com/light-bottom.css">

<!-- Begin Cookie Consent plugin. Original version by Silktide (http://silktide.com/cookieconsent), slightly modified by
explainthatstuff.com on July 29, 2015), released under GNU General Public License Version (http://www.gnu.org/licenses/gpl.txt) as published by the Free Software Foundation.
-->
<script type="text/javascript">
    window.cookieconsent_options = {"message":"Cookies help this site work smoothly. Unless you opt out, our partners may use them for advertising and social media. You can <a href=cookie-delete.html>opt in or out of cookies</a> at any time.","dismiss":"OK!","learnMore":"More info","link":"https://www.explainthatstuff.com/privacy.html","theme":"https://cdn4.explainthatstuff.com/light-bottom.css"}; 
</script>

<script type="text/javascript">
function downloadJSAtOnload() {
var element = document.createElement("script");
element.src = "https://cdn4.explainthatstuff.com/cookieconsent3.latest.min.js";
document.body.appendChild(element);
}
if (window.addEventListener)
window.addEventListener("load", downloadJSAtOnload, false);
else if (window.attachEvent)
window.attachEvent("onload", downloadJSAtOnload);
else window.onload = downloadJSAtOnload;
</script>
<!-- End Cookie Consent plugin -->

<style>
span.credit,span.slinks{font-size:80%}.sprite,img.fbfollow{width:25px;height:25px}h1.center,h2.grey,p.adtop,p.center{text-align:center}div.crumb,div.newsxpl h2,span.credit,span.italic{font-style:italic}h3,table h2{padding-top:0}p{padding:0}td.grey{background-color:silver}a:visited{color:purple}div.boxwarning h2{padding:5px;margin:0;background-color:#000;color:#ff6}div.alsoon a,div.alsoon a:hover{background:0 0;border:0;font-weight:700}div.alsoon a:hover{color:#fff;text-decoration:underline}a:link,div.alsoon a{text-decoration:none}div.alsoon a{color:#fff}table,td{border:1px solid #999}div.alsoon h2{color:#fff}span.bold{font-weight:700}span.slinks{color:#999}span.chem{font-size:75%;vertical-align:sub}span.super{font-size:75%;vertical-align:super}.sprite,img#appeal,img#bmk2{vertical-align:middle}span.equation{font-size:150%}span.question{float:right;color:#bbb;font-size:300px;line-height:100px}span.pre{font-family:Courier New,Courier,monospace}body,td{font-family:sans-serif}span.highlight{font-weight:700;color:#000}.sprite{padding:0;background:url(https://cdn4.explainthatstuff.com/sprite3.png) no-repeat}img#star,td{vertical-align:top}img.shr1{background-position:0 0}img.shr2{background-position:-25px 0}img.shr3{background-position:-50px 0}img.shr4{background-position:-75px 0}img.shr5{background-position:-100px 0}img.shr6{background-position:-125px 0}img.shr7{background-position:-150px 0}img.shr8{background-position:-175px 0}img.shr9{background-position:-200px 0}img.shr10{background-position:-225px 0}img.shr11{background-position:-250px 0}img.shr12{background-position:-275px 0}img.fbfollow{padding:0;background:url(https://cdn4.explainthatstuff.com/sprite3.png) -75px 0 no-repeat}img.footlogo{padding:0;background:url(https://cdn4.explainthatstuff.com/sprite3.png) 0 -136px no-repeat;width:200px;height:32px}img.atoms{padding:0;background:url(https://cdn4.explainthatstuff.com/sprite3.png) 0 -168px no-repeat;width:92px;height:150px}td,th{padding:1em}body{padding:0;margin:0 2px;background-color:navy;color:#222;line-height:1.4}table{margin:1em}th{border-style:solid;border-width:1px;background-color:#f6f6f6}td{background-color:#fff}h2.grey{background-color:#f0f0f0;font-weight:700;padding:.5em;border:1px solid #ccc}h2,h3{padding-top:1em}a.twitshr,a.faceshr,a.butred,a.butred:hover,a.button,a.button:hover{padding:.5em;font-weight:700}h1,h2,h3,h4{color:navy}a:link{color:#22f}a:link:hover{color:#22f;text-decoration:underline}a.twitshr,a.twitshr:hover,a.faceshr,a.faceshr:hover,a.butred,a.butred:hover,a.button,a.button:hover,a:visited:hover,div.header a:visited,div.header a:visited:hover,div.nav{text-decoration:none}a:visited:hover{color:purple}a.img:link:hover,a.img:visited:hover{color:#ff0;background-color:#fff}a.button{color:#fff;background-color:navy}a.button:hover{color:navy;background-color:#fff;border:2px solid navy}a.butred{color:#fff;background-color:red;border:1px solid #000}a.butred:hover{color:red;background-color:#fff;border:1px solid red}div.header{padding:0 1em;float:left;background-color:navy}div.alphabet{line-height:2;font-size:150%;color:#ccc}div.container{background-color:#fff;border-style:solid;border-width:0 1px;border-color:navy}div.searchtop{padding:.5em 1em 2em 2em;background-color:navy}div.crumb,div.nav{background-color:#ddd}div.adtop{padding:0 0 .5em;clear:both;text-align:center}div.adrightlg,div.adrightsm{padding:1em 0 1em 1em;float:right;font-size:90%;color:#999;text-align:center;margin-right:5%}div.adrightsm{width:310px;height:260px}div.adrightlg{width:336px;height:280px}div.cc{text-align:center;margin-left:5%;margin-right:5%;padding-bottom:2em;font-size:80%;color:#666}div.crumb{clear:both;padding-top:.3em;font-size:80%;color:#444;text-align:center}div.alsoon,div.footer,div.nav{font-size:90%;text-align:center;clear:both}div.advinfo2{text-align:center;padding:2em;font-weight:700}div.nav{padding:.5em;font-weight:700;border-width:0 0 1px;border-style:solid;border-color:#bbb}div.bottomsquare,div.main2{padding-bottom:1em;margin-left:5%;margin-right:5%}div.nav a,div.nav a:hover{color:#222;background:0 0;border:0;font-weight:700}div.nav a:hover{text-decoration:underline}div.alsoon,div.footer,div.nav a{text-decoration:none}div.main2{padding-top:5px}div.bottomsquare{padding-top:2em;padding-right:2em}div.box{padding:40px;background-color:#f8f8f8;border-color:#ddd;border-style:solid;border-width:1px;margin:2em 10%;box-shadow:10px 10px 5px #888}div.box h2{padding:5px;margin:0;background-color:#666;color:#fff;border:1px solid #ddd}div.boxwarning{padding:40px;border:5px solid #000;background-color:#ff6;margin-left:10%;margin-right:10%}div.sharesites{margin-top:2em;margin-left:5%;margin-bottom:2em}div.googlesearch{margin:2em 0 2em 5%}div.alsoon{padding:0 0 .5em;background:navy;font-weight:700;line-height:200%}div.footer{padding:1em;background-color:#ddd;font-weight:700}div.footer a,div.footer a:hover{color:#222;font-weight:700;background:0 0;border:0}div.footer a:hover{text-decoration:underline}div.footer a{text-decoration:none}div.footer li.inline{display:inline;border:solid #444;border-width:0 1px 0 0;padding:0 5px}div.tableft{float:left;margin:1em;width:40%}div.tabright{float:right;margin:1em;width:40%}div.tabclear{clear:both;border-width:0 0 1px;border-style:dashed;border-color:#ddd}div.newsxpl,div.pullquote{width:300px;border:1px solid #ccc;background-color:#eee;float:right}div.pullquote{margin:20px;text-align:center;font-size:120%}div.newsbox{width:300px;height:500px;float:right;padding:2em 0 2em 2em}div.newsxpl{margin:2em 0 2em 2em;padding:1em}div.newsxpl h2{padding:0;margin:0;color:#fff;text-align:center;width:100%;background-color:navy}div.newsxpl h3{padding:1em 0 0;margin:0}div.facebox{width:300px;height:300px;float:right;padding:0 0 0 1em}div.sharebot{padding-top:1em}div.sharebot li.shr{display:block;padding:0 0 5px}div.totop{clear:both;background-color:#ddd;text-align:right;font-size:90%;font-weight:700;text-decoration:none;color:navy;padding-right:.5em}div.totop a,div.totop a:hover{font-weight:700;background:0 0;border:0}div.totop a:hover{color:#222;text-decoration:underline}div.totop a{color:navy;text-decoration:none}div.citation{border:1px dotted #ddd;padding:.5em;width:90%}span.slinks2{color:#999}span.dropcap{float:left;color:#222;font-size:100px;line-height:70px;padding-top:2px;font-family:Times,serif,Georgia}li{padding:5px}li.narrow{padding:0}li.inline,li.inline2{padding:0 5px;display:inline}li.inline{border:solid #aaa;border-width:0 1px 0 0}li.shr{display:inline;padding:0 5px 0 0}ul.nobull{list-style-type:none;padding:0;margin:0}img.headlogo{padding:0;background:url(https://cdn4.explainthatstuff.com/sprite3.png) 0 -80px no-repeat;width:350px;height:51px}img.gplus{padding:0;background:url(https://cdn4.explainthatstuff.com/sprite3.png) -200px -64px no-repeat;width:16px;height:16px}img.tweet{padding:0;background:url(https://cdn4.explainthatstuff.com/sprite3.png) no-repeat;width:65px;height:22px;background-position:-216px -58px;}img.fblike{padding:0;background:url(https://cdn4.explainthatstuff.com/sprite3.png) no-repeat;width:66px;height:22px;background-position:-281px -58px;}img.srchbox{padding:0;background:url(https://cdn4.explainthatstuff.com/sprite3.png) -200px -25px no-repeat;width:146px;height:29px}img.srchboxsm{padding:0;background:url(https://cdn4.explainthatstuff.com/sprite3.png) -92px -168px no-repeat;width:91px;height:29px}img{border:none}img.icentno,img.ileftno,img.irightno,img.itopno{padding:1px;border:1px solid #999}img.ileft{float:left;margin:1em 1em 1em 0}img.iright{float:right;margin:1em 0 1em 1em}img.icentno{margin:.5em}img.ileftno{float:left;margin:1em 2em 1em 0}img.irightno{float:right;margin:1em 0 1em 2em}img.itopno{float:left;margin:1em 2em 1em 0}img#left,img.itop{float:left;margin:1em 1em 1em 0}img#right{float:right;margin:1em 0 1em 1em}img#ets{width:350px;height:56px}img#appeal{width:179px;height:37px}img#star{width:15px;height:16px;margin-right:2px}img#bmk2{width:120px;height:50px;margin-right:1em}a.faceshr:hover,a.twitshr:hover{color:#eee}a.twitshr{color:#fff;background-color:#55acee;border:1px solid #999}a.faceshr{color:#fff;background-color:#3b5998;border:1px solid #999}div.contentsbox{padding:0.5em 1em;margin:1em 2em 1em 0;background-color:#f8f8f8;border-color:#ddd;border-style:solid;border-width:1px;font-size:90%;width:30%;float:left;}div.contentsbox h3{padding:5px;margin:0;color:#222;}div.contentsbox ol{padding:0em 1em;}div.contentsbox ul{padding:0em;margin:0em;list-style-type:none;}
</style>
</head>

<body itemscope itemtype="http://schema.org/Article"><div class="container"><a name="pagetop"></a>









<div class="header"><IMG class="headlogo" ALT="Explain that Stuff" src="https://cdn4.explainthatstuff.com/dot.gif"></div>

<div class="searchtop">
<a href="searchmob.html" title="Site search" class="img"><IMG class="srchboxsm" ALT="Search" src="https://cdn4.explainthatstuff.com/dot.gif"></a>
</div>


<!-- Ad placement: computing -->



<div class="crumb">
You are here:
<a href="/">Home page</a> &gt;
<a href="articles_computers.html">Computers</A> &gt;
Computer history
</div>

<!-- old nav bar -->



<div class="nav">
<ul class="nobull"><li class="inline"><a href="/">Home</a></li>
<li class="inline"><a href="azindex.html">A-Z index</a></li>
<li class="inline"><a href="mybooks.php">Get the book</a></li>
<li class="inline"><a href="followus.html">Follow us</a></li>
<li class="inline"><a href="random.php">Random article</a></li>
<li class="inline"><a href="timeline.html">Timeline</a></li>
<li class="inline"><a href="teaching-guide.html">Teaching guide</a></li>
<li class="inline"><a href="aboutus.html">About us</a></li>
<li class="inline2"><a href="privacy.html">Privacy &amp; cookies</a></li>
</ul>
</div>

<div class="adrightsm">Advertisement
<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- ets_midright_300x250 -->
<ins class="adsbygoogle"
     style="display:inline-block;width:300px;height:250px"
     data-ad-client="ca-pub-1030585152417294"
     data-ad-slot="1317849019"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
</div>








<div class="main2">
<P><img itemprop="image" src="https://cdn4.explainthatstuff.com/nasa-pleiades-supercomputer.jpg" class="ileftno" width=300 height=226 alt="Pleiades ICE Supercomputer built from racks of Silicon Graphics workstations."></P>

<h1 itemprop="headline name">A brief history of computers</h1>

<div class="sharetop"><ul class="nobull">
<li class="shr"><iframe src="https://www.facebook.com/plugins/like.php?app_id=214994648542122&amp;href=http%3A%2F%2Fwww.explainthatstuff.com%2Fhistoryofcomputers.html&amp;send=false&amp;layout=button_count&amp;width=90&amp;show_faces=false&amp;action=like&amp;colorscheme=light&amp;font&amp;height=21" scrolling="no" frameborder="0" style="border:none; overflow:hidden; width:90px; height:21px;" allowTransparency="true"></iframe></li>
<!-- plusone and tweet -->
<!-- sharebuttons -->
<LI class="shr"><a target="_blank" rel="noopener" href="https://www.twitter.com/home?status=The+history+of+computers+https://www.explainthatstuff.com/historyofcomputers.html" title="Tweet this article"><IMG class="tweet" ALT="Tweet" src="https://cdn4.explainthatstuff.com/dot.gif"></a></LI>
</ul></div>

<p>by <a href="chris-woodford.html">Chris Woodford</a>. <i>Last updated: December 2, 2018.</i></p>
<meta itemprop="alternativeHeadline" content="History of computers">
<meta itemprop="description" content="A history of computers, from the abacus to the iPhone via Charles Babbage, the Eniac, Alan Turing, and other milestones.">
<meta itemprop="datePublished" content="2006-05-01">
<meta itemprop="dateModified" content="2018-12-02">


<p itemprop="articleBody"><span class="dropcap">C</span>omputers truly came into their own as
great <a href="inventors-and-inventions.html">inventions</a> in the last two decades of the 20th century. But their
history stretches back more than 2500 years to the abacus: a simple
<a href="calculators.html">calculator</A> made from beads and wires, which is still used in some parts
of the world today. The difference between an ancient abacus and a
modern computer seems vast, but the principle&#8212;making repeated
calculations more quickly than the human brain&#8212;is exactly the same.</p>
<p>Read on to learn more about the history of computers&#8212;or take a look at
our article on <a href="howcomputerswork.html">how computers work</a>.</p>

<p><span class="credit">Photo: One of the world's most powerful computers: NASA's Pleiades ICE <a href="how-supercomputers-work.html">supercomputer</a>
consists of 112,896 processor cores made from 185 racks of Silicon Graphics (SGI) workstations. Photo by Dominic Hart courtesy of
<a target="_blank" rel="noopener" href="https://www.nasa.gov/centers/ames/images/">NASA Ames Research Center</a>.</span></p>


<!-- No ad  -->

<div class="contentsbox">
<h3>Contents</h3>
<OL>
<LI><a href="#cogs">Cogs and Calculators</a></li>
<LI><a href="#engi">Engines of Calculation</a></li>
<LI><a href="#bush">Bush and the bomb</a></li>
<LI><a href="#turi">Turing&mdash;tested</a></li>
<LI><a href="#thef">The first modern computers</a></li>
<LI><a href="#them">The microelectronic revolution</a></li>
<LI><a href="#pers">Personal computers</a></li>
<LI><a href="#theu">The user revolution</a></li>
<LI><a href="#from">From nets to the Internet</a></li>
<LI><a href="#fomo">Find out more</a></li>
</OL>
</div>

<a name="cogs"></A><h2>Cogs and Calculators</h2>

<p>It is a measure of the brilliance of the abacus, invented in the
Middle East circa 500 BC, that it remained the fastest form of
calculator until the middle of the 17th century. Then, in 1642, aged
only 18, French scientist and philosopher <span class="bold">Blaise
Pascal</span> (1623&ndash;1666) invented the first practical mechanical
<a href="calculators.html">calculator</A>, the Pascaline, to help his tax-collector father do his
sums. The machine had a series of interlocking cogs (<a
 href="gears.html">gear</a> wheels with teeth around their
outer edges) that could add and subtract decimal numbers. Several
decades later, in 1671, German mathematician and philosopher <span
 class="bold">Gottfried Wilhelm Leibniz</span> (1646&ndash;1716) came up with
a similar but more advanced machine. Instead of using cogs, it had a
"stepped drum" (a cylinder with teeth of increasing length around its
edge), an innovation that survived in mechanical calculators for 300
hundred years. The Leibniz machine could do much more than Pascal's: as
well as adding and subtracting, it could multiply, divide, and work out
square roots. Another pioneering feature was the first <a href="how-computer-memory-works.html">memory</a>
store or "register."</p>

<p>Apart from developing one of the world's earliest mechanical
calculators, Leibniz is remembered for another important contribution
to computing: he was the man who invented binary code, a way of
representing any decimal number using only the two digits zero and one.
Although Leibniz made no use of binary in his own calculator, it set
others thinking. In 1854, a little over a century after Leibniz had died, Englishman 
<span class="bold">George Boole</span> (1815&ndash;1864) used the idea to invent a
new branch of mathematics called Boolean algebra.
<a name="1"></a><a href="#1fn">[1]</a> In modern computers,
binary code and Boolean algebra allow computers to make simple
decisions by comparing long strings of zeros and ones. But, in the 19th
century, these ideas were still far ahead of their time. It would take
another 50&ndash;100 years for mathematicians and computer scientists to figure out how
to use them (find out more in our articles about <a href="calculators.html">calculators</A>
and <a href="logicgates.html">logic gates</A>).</p>

<P class="center"><img src="https://cdn4.explainthatstuff.com/blaise-pascal-pascaline-detail.jpg" class="icentno" width=600 height=300 alt="Details of the user interface and internal mechanism of Blaise Pascal's Pascaline."></P>
<p><span class="credit">Artwork: Pascaline: Two details of Blaise Pascal's 17th-century calculator. 
Left: The "user interface": the part where you dial in numbers you want to calculate. Right: The internal gear mechanism. Picture courtesy of <a href="http://www.loc.gov/pictures/item/2006690493/" target="_blank" rel="noopener">US Library of Congress</A>.</span></p>


<a name="engi"></A><h2>Engines of Calculation</h2>

<p>Neither the abacus, nor the mechanical calculators constructed by
Pascal and Leibniz really qualified as computers. A calculator is a
device that makes it quicker and easier for people to do sums&#8212;but it
needs a human operator. A computer, on the other hand, is a machine
that can operate automatically, without any human help, by following a
series of stored instructions called a program (a kind of mathematical
recipe). Calculators evolved into computers when people devised ways of
making entirely automatic, programmable calculators.</p>

<P><img src="https://cdn4.explainthatstuff.com/herman-hollerith-statistics-patent.png" class="irightno" width=300 height=524 alt="How punched cards were used in early computers. A drawing from Herman Hollerith's Art of Compiling Statistics Patent, January 8, 1889."></P>
<p><span class="credit">Photo: Punched cards: Herman Hollerith perfected the way of using punched cards
and paper tape to store information and feed it into a machine. Here's a drawing from his 1889 patent
<a href="https://patents.google.com/patent/US395782" target="_blank" rel="noopener">Art of Compiling Statistics</A> (US Patent#395,782),
showing how a strip of paper (yellow) is punched with different patterns of holes (orange) that correspond to
statistics gathered about people in the US census. Picture courtesy of US Patent and Trademark Office.</span></p>

<p>The first person to attempt this was a rather obsessive, notoriously
grumpy English mathematician named <span class="bold">Charles Babbage</span>
(1791&ndash;1871). Many regard Babbage as the "father of the computer"
because his machines had an input (a way of feeding in numbers), a
memory (something to store these numbers while complex calculations
were taking place), a processor (the number-cruncher that carried out
the calculations), and an output (a printing mechanism)&#8212;the same basic
components shared by all modern computers. During his lifetime, Babbage
never completed a single one of the hugely ambitious machines that he
tried to build. That was no surprise. Each of his programmable
"engines" was designed to use tens of thousands of precision-made
gears. It was like a pocket watch scaled up to the size of a <a href="steamengines.html">steam
engine</A>, a Pascal or Leibniz machine magnified a thousand-fold in
dimensions, ambition, and complexity. For a time, the British
government financed Babbage&#8212;to the tune of &pound;17,000, then an
enormous sum. But when Babbage pressed the government for more money to
build an even more advanced machine, they lost patience and pulled out.
Babbage was more fortunate in receiving help from <span class="bold">Augusta
Ada Byron</span> (1815&ndash;1852), Countess of Lovelace, daughter of the
poet Lord Byron. An enthusiastic mathematician, she helped to refine
Babbage's ideas for making his machine programmable&#8212;and this is why she
is still, sometimes, referred to as the world's first computer
programmer.
<a name="2"></a><a href="#2fn">[2]</a> Little of Babbage's work survived after his death. But
when, by chance, his notebooks were rediscovered in the 1930s, computer
scientists finally appreciated the brilliance of his ideas.
Unfortunately, by then, most of these ideas had already been reinvented
by others.</p>


<p>Babbage had intended that his machine would take the drudgery out of
repetitive calculations. Originally, he imagined it would be used by
the army to compile the tables that helped their gunners to fire
cannons more accurately. Toward the end of the 19th century, other
inventors were more successful in their effort to construct "engines"
of calculation. American statistician <span class="bold">Herman
Hollerith</span> (1860&ndash;1929) built one of the world's first practical
calculating machines, which he called a tabulator, to help compile
census data. Then, as now, a census was taken each decade but, by the
1880s, the population of the United States had grown so much through
immigration that a full-scale analysis of the data by hand was taking
seven and a half years. The statisticians soon figured out that, if
trends continued, they would run out of time to compile one census
before the next one fell due. Fortunately, Hollerith's tabulator was an
amazing success: it tallied the entire census in only six weeks and
completed the full analysis in just two and a half years. Soon
afterward, Hollerith realized his machine had other applications, so he
set up the Tabulating Machine Company in 1896 to manufacture it
commercially. A few years later, it changed its name to the
Computing-Tabulating-Recording (C-T-R) company and then, in 1924,
acquired its present name: International Business Machines (IBM).</p>

<a name="bush"></A><h2>Bush and the bomb</h2>
<p>The history of computing remembers colorful characters like Babbage,
but others who played important&#8212;if supporting&#8212;roles are less well
known. At the time when C-T-R was becoming IBM, the world's most
powerful calculators were being developed by US government scientist <span
 class="bold">Vannevar Bush</span> (1890&ndash;1974). In 1925, Bush made the
first of a series of unwieldy contraptions with equally cumbersome
names: the New Recording Product Integraph Multiplier. Later, he built
a machine called the Differential Analyzer, which used gears, belts,
levers, and shafts to represent numbers and carry out calculations in a
very physical way, like a gigantic mechanical slide rule. Bush's
ultimate calculator was an improved machine named the Rockefeller
Differential Analyzer, assembled in 1935 from 320 km (200 miles) of
wire and 150 <a href="electricmotors.html">electric motors</a>. Machines
like these were known as <a href="analog-and-digital.html">analog</a> calculators&#8212;analog because they stored
numbers in a physical form (as so many turns on a wheel or twists of a
belt) rather than as digits. Although they could carry out incredibly
complex calculations, it took several days of wheel cranking and belt
turning before the results finally emerged. </p>

<P><img src="https://cdn4.explainthatstuff.com/differentialanalyzer.jpg" class="ileftno" width=300 height=240  alt="Photo of Differential Analyzer c.1951 by NASA"></P>

<p><span class="credit">Photo: A Differential Analyzer. The black part in the
background is the main part of the machine. The operator sits at a
smaller console in the foreground.
Picture courtesy of <a target="_blank" rel="noopener"
 href="https://www.flickr.com/photos/nasacommons/7538107210">NASA on the Commons</a>
(where you can download a larger version of this photo).</span></p>
<!--end picture-->
<p>Impressive machines like the Differential Analyzer were only one of
several outstanding contributions Bush made to 20th-century technology.
Another came as the teacher of <span class="bold">Claude Shannon</span>
(1916&ndash;2001), a brilliant mathematician who figured out how electrical
circuits could be linked together to process binary code with Boolean
algebra (a way of comparing binary numbers using logic) and thus make
simple decisions. During World War II, President Franklin D. Roosevelt
appointed Bush chairman first of the US National Defense Research
Committee and then director of the Office of Scientific Research and
Development (OSRD). In this capacity, he was in charge of the Manhattan
Project, the secret $2-billion initiative that led to the creation of
the atomic bomb. One of Bush's final wartime contributions was to
sketch out, in 1945, an idea for a memory-storing and sharing device
called Memex that would later inspire Tim Berners-Lee to invent the
<a href="howthewebworks.html">World Wide Web</a>.
<a name="3"></a><a href="#3fn">[3]</a> Few outside the world
of computing remember Vannevar Bush today&#8212;but what a legacy! As a
father of the digital computer, an overseer of the atom bomb, and an
inspiration for the Web, Bush played a pivotal role in three of the
20th-century's most far-reaching technologies.</p>

<a name="turi"></A><h2>Turing&mdash;tested</h2>
<P>
Many of the pioneers of computing were hands-on experimenters&mdash;but by no means all of them.
One of the key figures in the history of 20th-century computing, <span class="bold">Alan Turing</span> (1912&ndash;1954) was
a brilliant Cambridge mathematician whose major contributions were to the <span class="italic">theory</span> of how
computers processed information. In 1936, at the age of just 23, Turing wrote
a groundbreaking mathematical paper called "On computable numbers, with an application to the Entscheidungsproblem,"
in which he described a theoretical computer now known as a
Turing machine (a simple information processor that works through a series of instructions,
reading data, writing results, and then moving on to the next instruction).
Turing's ideas were hugely influential in the years that followed and many people
regard him as the father of modern computing&mdash;the 20th-century's equivalent of Babbage.</P>

<P>
Although essentially a theoretician, Turing did get involved with real, practical machinery,
unlike many mathematicians of his time. During World War II, he played a pivotal role in the development of code-breaking
machinery that, itself, played a key part in Britain's wartime victory; later, he played a lesser role
in the creation of several large-scale experimental computers including
ACE (Automatic Computing Engine), Colossus, and the Manchester/Ferranti Mark I (described below).
Today, Alan Turing is best known for conceiving what's become known as the Turing test, a simple
way to find out whether a computer can be considered intelligent by seeing whether it can
sustain a plausible conversation with a real human being.</P>

<a name="thef"></A><h2>The first modern computers</h2>

<p>The World War II years were a crucial period in the history of
computing, when powerful gargantuan computers began to appear. Just
before the outbreak of the war, in 1938, German engineer <span
 class="bold">Konrad Zuse</span> (1910&ndash;1995) constructed his Z1, the
world's first programmable binary computer, in his parents' living
room.
<a name="4"></a><a href="#4fn">[4]</a> The following year, American physicist <span class="bold">John
Atanasoff</span> (1903&ndash;1995) and his assistant, electrical engineer <span
 class="bold">Clifford Berry</span> (1918&ndash;1963), built a more elaborate
binary machine that they named the Atanasoff Berry Computer (ABC). It
was a great advance&#8212;1000 times more accurate than Bush's Differential
Analyzer. These were the first machines that used
electrical switches to store numbers: when a switch was "off", it
stored the number zero; flipped over to its other, "on", position, it
stored the number one. Hundreds or thousands of switches could thus
store a great many binary digits (although binary is much less
efficient in this respect than decimal, since it takes up to eight
binary digits to store a three-digit decimal number). These machines
were digital computers: unlike analog machines, which stored numbers
using the positions of wheels and rods, they stored numbers as digits. </p>
<p>The first large-scale digital computer of this kind appeared in 1944
at Harvard University, built by mathematician <span class="bold">Howard
Aiken</span> (1900&ndash;1973). Sponsored by IBM, it was variously known as
the Harvard Mark I or the IBM Automatic Sequence Controlled Calculator
(ASCC). A giant of a machine, stretching 15m (50ft) in length, it was
like a huge mechanical calculator built into a wall. It must have <i>sounded</i>
impressive, because it stored and processed numbers using
"clickety-clack" electromagnetic <a href="howrelayswork.html">relays</A> (electrically operated 
<a href="magnetism.html">magnets</a>
that automatically switched lines in <a href="telephone.html">telephone</a> exchanges)&#8212;no fewer than
3304 of them. Impressive they may have been, but relays suffered from
several problems: they were large (that's why the Harvard Mark I had to
be so big); they needed quite hefty pulses of power to make them switch;
and they were slow (it took time for a relay to flip from "off" to "on"
or from 0 to 1).</p>
<!--picture goes here-->

<p><img src="https://cdn4.explainthatstuff.com/analogcomputer.jpg" class="irightno" width=357 height=285 alt="Photo of analog computer c.1949 by NASA"></P>

<p><span class="credit">Photo: An analog computer being used in military
research in 1949. Picture courtesy of 
<a target="_blank" rel="noopener"
 href="https://www.flickr.com/photos/nasacommons/9467782090/">NASA on the Commons</a> (where you can download a larger version.</span></p>
<!--end picture-->
<p>Most of the machines developed around this time were intended for
military purposes. Like Babbage's never-built mechanical engines, they
were designed to calculate artillery firing tables and chew through the
other complex chores that were then the lot of military mathematicians.
During World War II, the military co-opted thousands of the best
scientific minds: recognizing that science would win the war, Vannevar
Bush's Office of Scientific Research and Development employed 10,000
scientists from the United States alone. Things were very different in
Germany. When Konrad Zuse offered to build his Z2 computer to help the
army, they couldn't see the need&#8212;and turned him down.</p>
<p>On the Allied side, great minds began to make great breakthroughs.
In 1943, a team of mathematicians based at Bletchley Park near London,
England (including Alan Turing) built a computer called Colossus to help them crack secret
German codes. Colossus was the first fully <a href="electronics.html">electronic</a> computer. Instead
of relays, it used a better form of switch known as a vacuum tube (also
known, especially in Britain, as a valve). The vacuum tube, each one about
as big as a person's thumb and glowing red hot like a tiny electric
light bulb, had been invented in 1906 by <span class="bold">Lee de
Forest</span> (1873&ndash;1961), who named it the Audion. This
breakthrough earned de Forest his nickname as "the father of radio" because their first major
use was in <a href="radio.html">radio receivers</a>, where they <a href="amplifiers.html">amplified</a> weak incoming signals
so people could hear them more clearly.
<a name="5"></a><a href="#5fn">[5]</a>  In computers such as the ABC
and Colossus, vacuum tubes found an alternative use as faster and more
compact switches. </p>
<p>Just like the codes it was trying to crack, Colossus was top-secret
and its existence wasn't confirmed until after the war ended. As far as
most people were concerned, vacuum tubes were pioneered by a more
visible computer that appeared in 1946: the Electronic Numerical
Integrator And Calculator (ENIAC). The ENIAC's inventors, two
scientists from the University of Pennsylvania, <span class="bold">John
Mauchly</span> (1907&ndash;1980) and <span class="bold">J. Presper Eckert</span>
(1919&ndash;1995), were originally inspired by Bush's Differential Analyzer;
years later Eckert recalled that ENIAC was the "descendant of Dr Bush's
machine." But the machine they constructed was far more ambitious. It
contained nearly 18,000 vacuum tubes (nine times more than Colossus),
was around 24 m (80 ft) long, and weighed almost 30 tons. ENIAC is
generally recognized as the world's first fully electronic,
general-purpose, digital computer. Colossus might have qualified for
this title too, but it was designed purely for one job (code-breaking);
since it couldn't store a program, it couldn't easily be reprogrammed to do other things.</p>

<p>ENIAC was just the beginning. Its two inventors formed the Eckert
Mauchly Computer Corporation in the late 1940s. Working with a
brilliant Hungarian mathematician, <span class="bold">John von Neumann</span>
(1903&ndash;1957), who was based at Princeton University, they then designed
a better machine called EDVAC (Electronic Discrete Variable Automatic
Computer). In a key piece of work, von Neumann helped to define how the
machine stored and processed its programs, laying the foundations for
how all modern computers operate.
<a name="6"></a><a href="#6fn">[6]</a>  
After EDVAC, Eckert and Mauchly developed UNIVAC 1 (UNIVersal Automatic Computer) in 1951. They were
helped in this task by a young, largely unknown American mathematician
and Naval reserve named <span class="bold">Grace Murray Hopper</span>
(1906&ndash;1992), who had originally been employed by Howard Aiken on the
Harvard Mark I. Like Herman Hollerith's tabulator over 50 years before,
UNIVAC 1 was used for processing data from the US census. It was then
manufactured for other users&#8212;and became the world's first large-scale
commercial computer. </p>
<p>Machines like Colossus, the ENIAC, and the Harvard Mark I compete
for significance and recognition in the minds of computer historians.
Which one was truly the first great modern computer? All of them and
none: these&#8212;and several other important machines&#8212;evolved our idea of
the modern electronic computer during the key period between the late
1930s and the early 1950s. Among those other machines were pioneering
computers put together by English academics, notably the Manchester/Ferranti Mark I,
built at Manchester University by <span class="bold">Frederic Williams</span>
(1911&ndash;1977) and <span class="bold">Thomas Kilburn</span> (1921&ndash;2001),
and the EDSAC (Electronic Delay Storage Automatic Calculator), built by
<span class="bold">Maurice Wilkes</span> (1913&ndash;2010) at Cambridge
University.
<a name="7"></a><a href="#7fn">[7]</a></p>

<a name="them"></A><h2>The microelectronic revolution</h2>

<p>Vacuum tubes were a considerable advance on relay switches, but
machines like the ENIAC were notoriously unreliable. The modern term
for a problem that holds up a computer program is a "bug." Popular
legend has it that this word entered the vocabulary of computer
programmers sometime in the 1950s when moths, attracted by the glowing
lights of vacuum tubes, flew inside machines like the ENIAC, caused a
short circuit, and brought work to a juddering halt. But there were
other problems with vacuum tubes too. They consumed enormous amounts of
power: the ENIAC used about 2000 times as much electricity as a modern
laptop. And they took up huge amounts of space. Military needs were
driving the development of machines like the ENIAC, but the sheer size
of vacuum tubes had now become a real problem. ABC had used 300 vacuum
tubes, Colossus had 2000, and the ENIAC had 18,000. The ENIAC's
designers had boasted that its calculating speed was "at least 500
times as great as that of any other existing computing machine." But
developing computers that were an order of magnitude more powerful
still would have needed hundreds of thousands or even millions of
vacuum tubes&#8212;which would have been far too costly, unwieldy, and
unreliable. So a new technology was urgently required. </p>

<p><img src="https://cdn4.explainthatstuff.com/fettransistor.jpg" class="ileftno" height="240" width="300" ALT="A FET transistor on a printed circuit board."></p>


<P><span class="credit">Photo: A typical transistor on an electronic circuit board.</span></P>

<p>The solution appeared in 1947 thanks to three physicists working at
Bell Telephone Laboratories (Bell Labs). <span class="bold">John
Bardeen</span> (1908&ndash;1991), <span class="bold">Walter Brattain</span>
(1902&ndash;1987), and <span class="bold">William Shockley</span> (1910&ndash;1989)
were then helping Bell to develop new technology for the American
public telephone system, so the electrical signals that carried phone calls could be
amplified more easily and carried further. Shockley, who was leading the team, believed
he could use semiconductors (materials such as germanium and silicon
that allow electricity to flow through them only when they've been
treated in special ways) to make a better form of <a href="amplifiers.html">amplifier</a> than the vacuum tube.
When his early experiments failed, he set Bardeen and Brattain to work
on the task for him. Eventually, in December 1947, they created a new
form of amplifier that became known as the point-contact transistor.
Bell Labs credited Bardeen and Brattain with the transistor and awarded
them a patent. This enraged Shockley and prompted him to invent an even
better design, the junction transistor, which has formed the basis of
most <a href="howtransistorswork.html">transistors</a> ever since.</p>
<p>Like vacuum tubes, transistors could be used as amplifiers or as
switches. But they had several major advantages. They were a fraction
the size of vacuum tubes (typically about as big as a pea), used no
power at all unless they were in operation, and were virtually 100
percent reliable. The transistor was one of the most important
breakthroughs in the history of computing and it earned its inventors
the world's greatest science prize, the
<a href="https://www.nobelprize.org/prizes/physics/1956/summary/" target="_blank" rel="noopener">1956 Nobel Prize in Physics</A>.
By that time, however, the three men had already gone their
separate ways. John Bardeen had begun pioneering research into
<a href="superconductors.html">superconductivity</a>, which would earn him a second Nobel Prize in 1972.
Walter Brattain moved to another part of Bell Labs. </p>
<p>William Shockley decided to stick with the transistor, eventually
forming his own corporation to develop it further. His decision would
have extraordinary consequences for the computer industry. With a small
amount of capital, Shockley set about hiring the best brains he could
find in American universities, including young electrical engineer
<span class="bold">Robert Noyce</span> (1927&ndash;1990) and
research chemist <span class="bold">Gordon Moore</span> (1929&ndash;).
It wasn't long before Shockley's idiosyncratic and bullying management
style upset his workers. In 1956, eight of them&#8212;including Noyce and
Moore&#8212;left Shockley Transistor to found a company of their own,
Fairchild Semiconductor, just down the road. Thus began the growth of
"Silicon Valley," the part of California centered on Palo Alto, where
many of the world's leading computer and electronics companies have
been based ever since.
<a name="8"></a><a href="#8fn">[8]</a></p>
<p>It was in Fairchild's California building that the next breakthrough
occurred&#8212;although, somewhat curiously, it also happened at exactly the
same time in the Dallas laboratories of Texas Instruments. In Dallas, a
young engineer from Kansas named <span class="bold">Jack Kilby</span>
(1923&ndash;2005) was considering how to improve the transistor. Although
transistors were a great advance on vacuum tubes, one key problem
remained. Machines that used thousands of transistors still had to be
hand wired to connect all these components together. That process was
laborious, costly, and error prone. Wouldn't it be better, Kilby
reflected, if many transistors could be made in a single package? This
prompted him to invent the "monolithic" <a href="integratedcircuits.html">integrated circuit (IC)</A>, a
collection of transistors and other components that could be
manufactured all at once, in a block, on the surface of a
semiconductor. Kilby's invention was another step forward, but it also
had a drawback: the components in his integrated circuit still had to
be connected by hand. While Kilby was making his breakthrough in
Dallas, unknown to him, Robert Noyce was perfecting almost exactly the
same idea at Fairchild in California. Noyce went one better, however:
he found a way to include the connections between components in an
integrated circuit, thus automating the entire process.</p>

<p><img src="https://cdn4.explainthatstuff.com/integratedcircuitnasa.jpg" class="irightno" height="288" width="327" ALT="Inside a typical microchip. You can see the  integrated circuit and the wires that connect to the terminals around its edge."></p>


<p><span class="credit">Photo: An integrated circuit seen from the inside. Photo by courtesy of <a href="https://www.nasa.gov/centers/glenn/multimedia/" target="_blank" rel="noopener">NASA Glenn Research Center (NASA-GRC)</A>.</span></p>

<p>Integrated circuits, as much as transistors, helped to shrink
computers during the 1960s. In 1943, IBM boss Thomas Watson had
reputedly quipped: "I think there is a world market for about five
computers." Just two decades later, the company and its competitors had
installed around 25,000 large computer systems across the United
States. As the 1960s wore on, integrated circuits became increasingly
sophisticated and compact. Soon, engineers were speaking of large-scale
integration (LSI), in which hundreds of components could be crammed
onto a single chip, and then very large-scale integrated (VLSI), when
the same chip could contain thousands of components.</p>
<p>The logical conclusion of all this miniaturization was that,
someday, someone would be able to squeeze an entire computer onto a
chip. In 1968, Robert Noyce and Gordon Moore had left Fairchild to
establish a new company of their own. With integration very much in
their minds, they called it Integrated Electronics or Intel for short.
Originally they had planned to make memory chips, but when the company
landed an order to make chips for a range of pocket calculators,
history headed in a different direction. A couple of their engineers, 
<span class="bold">Federico Faggin</span> (1941&ndash;) and
<span class="bold">Marcian Edward (Ted) Hoff</span> (1937&ndash;), realized that
instead of making a range of specialist chips for a range of
calculators, they could make a universal chip that could be programmed to work in
them all. Thus was born the general-purpose, single chip computer or
microprocessor&#8212;and that brought about the next phase of the computer
revolution.</p>
<a name="pers"></A><h2>Personal computers</h2>
<p>By 1974, Intel had launched a popular microprocessor known as the
8080 and computer hobbyists were soon building home computers around it.
The first was the MITS Altair 8800, built by <span class="bold">Ed
Roberts</span>. With its front panel covered in red 
<a href="diodes.html">LED</a> lights and
toggle switches, it was a far cry from modern PCs and laptops. Even so,
it sold by the thousand and earned Roberts a fortune. The Altair
inspired a Californian electronics wizard name <span class="bold">Steve
Wozniak</span> (1950&ndash;) to develop a computer of his own. "Woz" is often
described as the hacker's "hacker"&#8212;a technically brilliant and highly
creative engineer who pushed the boundaries of computing largely for
his own amusement. In the mid-1970s, he was working at the
Hewlett-Packard computer company in California, and spending his free
time tinkering away as a member of the Homebrew Computer Club in the
Bay Area. </p>

<p>After seeing the Altair, Woz used a 6502 microprocessor (made by an
Intel rival, Mos Technology) to build a better home computer of his
own: the Apple I. When he showed off his machine to his colleagues at
the club, they all wanted one too. One of his friends, <span
 class="bold">Steve Jobs</span> (1955&ndash;2011), persuaded Woz that they should
go into business making the machine. Woz agreed so, famously, they set
up Apple Computer Corporation in a garage belonging to Jobs' parents.
After selling 175 of the Apple I for the devilish price of $666.66, Woz
built a much better machine called the Apple ][ (pronounced "Apple
Two"). While the Altair 8800 looked like something out of a science
lab, and the Apple I was little more than a bare circuit board, the
Apple ][ took its inspiration from such things as Sony televisions and
stereos: it had a neat and friendly looking cream plastic case.
Launched in April 1977, it was the world's first
easy-to-use home "microcomputer." Soon home users, schools, and small
businesses were buying the machine in their tens of thousands&#8212;at $1298
a time. Two things turned the Apple ][ into a really credible machine
for small firms: a <a href="harddrive.html">disk drive</A> unit, launched in 1978, which made it
easy to store data; and a spreadsheet program called VisiCalc, which
gave Apple users the ability to analyze that data. In just two and a
half years, Apple sold around 50,000 of the machine, quickly
accelerating out of Jobs' garage to become one of the world's biggest
companies. Dozens of other microcomputers were launched around this
time, including the TRS-80 from Radio Shack (Tandy in the UK) and the
Commodore PET.
<a name="9"></a><a href="#9fn">[9]</a></p>
<!--picture goes here-->
<P class="center">
<img src="https://cdn4.explainthatstuff.com/appleII.jpg" width=300 height=225 class="icentno" alt="Apple ][ microcomputer in a museum glass case">
<img src="https://cdn4.explainthatstuff.com/sinclair-zx81-computer.jpg" width=283 height=225 class="icentno" alt="Sinclair ZX81 microcomputer">
<BR>
<p><span class="credit">Photos: Microcomputers&mdash;the first PCs. The Apple ][ and The 
<a href="https://en.wikipedia.org/wiki/ZX81" target="-new">Sinclair ZX81</A>, a build-it-yourself
microcomputer that became hugely popular in the UK when it was launched in 1981.
Both of these machines live in glass cases at Think Tank, the science museum in Birmingham, England.</span></p>
<!--end picture-->
<p>Apple's success selling to businesses came as a great shock to IBM
and the other big companies that dominated the computer industry. It
didn't take a VisiCalc spreadsheet to figure out that, if the trend
continued, upstarts like Apple would undermine IBM's immensely lucrative
business market selling "Big Blue" computers. In 1980, IBM finally
realized it had to do something and launched a highly streamlined
project to save its business. One year later, it released the IBM
Personal Computer (PC), based on an Intel 8080 microprocessor, which
rapidly reversed the company's fortunes and stole the market back from
Apple. </p>
<p>The PC was successful essentially for one reason. All the dozens of
microcomputers that had been launched in the 1970s&#8212;including the Apple
][&#8212;were incompatible. All used different hardware and worked in
different ways. Most were programmed using a simple, English-like
language called BASIC, but each one used its own flavor of BASIC, which
was tied closely to the machine's hardware design. As a result,
programs written for one machine would generally not run on another one
without a great deal of conversion. Companies who wrote software
professionally typically wrote it just for one machine and,
consequently, there was no software industry to speak of.</p>
<p>In 1976, <span class="bold">Gary Kildall</span> (1942&ndash;1994), a
teacher and computer scientist, and one of the founders of the Homebrew
Computer Club, had figured out a solution to this problem. Kildall
wrote an operating system (a computer's fundamental control software)
called CP/M that acted as an intermediary between the user's programs
and the machine's hardware. With a stroke of genius, Kildall realized
that all he had to do was rewrite CP/M so it worked on each different
machine. Then all those machines could run identical user
programs&#8212;without any modification at all&#8212;inside CP/M. That would make
all the different microcomputers compatible at a stroke. By the early
1980s, Kildall had become a multimillionaire through the success of his
invention: the first personal computer operating system. Naturally, 
when IBM was developing its personal computer, it
approached him hoping to put CP/M on its own machine. Legend has it
that Kildall was out flying his personal plane when IBM called, so
missed out on one of the world's greatest deals. But the truth seems to
have been that IBM wanted to buy CP/M outright for just $200,000, while
Kildall recognized his product was worth millions more and refused to
sell. Instead, IBM turned to a young programmer named <span class="bold">Bill Gates</span> (1955&ndash;).
His then tiny company, Microsoft, rapidly put together an operating system called DOS,
based on a product called QDOS (Quick and Dirty Operating System), which they acquired
from Seattle Computer Products. Some believe Microsoft and IBM cheated Kildall
out of his place in computer history; Kildall himself accused them of copying his ideas.
Others think Gates was simply the shrewder businessman. Either way, the IBM PC, powered by Microsoft's operating system,
was a runaway success.</p>

<p>Yet IBM's victory was short-lived. Cannily, Bill Gates had sold IBM the
rights to one flavor of DOS (PC-DOS) and retained the rights to a very
similar version (MS-DOS) for his own use. When other computer
manufacturers, notably Compaq and Dell, starting making IBM-compatible
(or "cloned") hardware, they too came to Gates for the software. IBM
charged a premium for machines that carried its badge, but consumers
soon realized that PCs were commodities: they contained almost
identical components&#8212;an Intel microprocessor, for example&#8212;no matter
whose name they had on the case. As IBM lost market share, the ultimate
victors were Microsoft and Intel, who were soon supplying the software
and hardware for almost every PC on the planet. Apple, IBM, and Kildall
made a great deal of money&#8212;but all failed to capitalize decisively on
their early success.
<a name="10"></a><a href="#10fn">[10]</a></p>

<!--picture goes here-->
<P><img src="https://cdn4.explainthatstuff.com/mainframecomputer.jpg" class="ileftno" width=350 height=187
 alt="Photo of mainframe computer c.1990 by NASA"></P>
<p><span class="credit">Photo: Personal computers threatened companies making
large "mainframes" like this one.
Picture courtesy of <a target="_blank" rel="noopener"
 href="https://www.flickr.com/photos/nasacommons/9467782298">NASA on the Commons</a>
(where you can download a larger version).</span></p>
<!--end picture-->

<a name="theu"></A><h2>The user revolution</h2>
<p>Fortunately for Apple, it had another great idea. One of the Apple
II's strongest suits was its sheer "user-friendliness." For Steve Jobs,
developing truly easy-to-use computers became a personal mission in the
early 1980s. What truly inspired him was a visit to PARC (Palo Alto
Research Center), a cutting-edge computer laboratory then run as a
division of the Xerox Corporation. Xerox had started developing
computers in the early 1970s, believing they would make <a href="papermaking.html">paper</A>
(and the highly lucrative <a href="photocopier.html">photocopiers</A> Xerox made) obsolete. One of PARC's
research projects was an advanced $40,000 computer called the Xerox
Alto. Unlike most microcomputers launched in the 1970s, which were
programmed by typing in text commands, the Alto had a desktop-like
screen with little picture icons that could be moved around with a mouse: it was
the very first graphical user interface (GUI, pronounced "gooey")&#8212;an
idea conceived by
<span class="bold">Alan Kay</span> (1940&ndash;) and now used in virtually
every modern computer. The Alto borrowed some of its ideas, including
the <a href="computermouse.html">mouse</a>, from 1960s computer
pioneer <span class="bold">Douglas Engelbart</span> (1925&ndash;2013).</p>
<p>Back at Apple, Jobs launched his own version of the Alto project to
develop an easy-to-use computer called PITS (Person In The Street).
This machine became the Apple Lisa, launched in January 1983&#8212;the first
widely available computer with a GUI desktop. With a retail price of
$10,000, over three times the cost of an IBM PC, the Lisa was a
commercial flop. But it paved the way for a better, cheaper machine
called the Macintosh that Jobs unveiled a year later, in January 1984. 
With its memorable launch ad for the Macintosh inspired by George Orwell's novel <span class="italic">1984</span>,
and directed by Ridley Scott (director of the dystopic movie <span class="italic">Blade Runner</span>), 
Apple took a swipe at IBM's monopoly, criticizing what it portrayed as
the firm's domineering&#8212;even totalitarian&#8212;approach: Big Blue was really
Big Brother. Apple's ad promised a very different vision: "On January 24, Apple Computer will introduce Macintosh. And you'll see why 1984 won't be like '1984'."
The Macintosh was a critical success and helped to invent
the new field of desktop publishing in the mid-1980s, yet it never came
close to challenging IBM's position.</p>
<p>Ironically, Jobs' easy-to-use machine also helped Microsoft to
dislodge IBM as the world's leading force in computing. When Bill Gates
saw how the Macintosh worked, with its easy-to-use picture-icon
desktop, he launched Windows, an upgraded version of his MS-DOS
software. Apple saw this as blatant plagiarism and filed a $5.5 billion
copyright lawsuit in 1988. Four years later, the case collapsed with
Microsoft effectively securing the right to use the Macintosh "look and
feel" in all present and future versions of Windows. Microsoft's
Windows 95 system, launched three years later, had an easy-to-use,
Macintosh-like desktop and MS-DOS running behind the scenes.</p>

<P><img src="https://cdn4.explainthatstuff.com/ibm-blue-gene-supercomputer.jpg" class="irightno" width=300 height=199 alt="Photo of IBM Blue Gene supercomputer at Argonne National Laboratory."></P>
<p><span class="credit">Photo: The IBM Blue Gene/P <a href="how-supercomputers-work.html">supercomputer</a> at
Argonne National Laboratory: one of the world's most powerful
computers. Picture courtesy of Argonne National Laboratory published on
<a target="_blank" rel="noopener" href="https://www.flickr.com/photos/argonne/3323018571/">Flickr</a> in 2009
under a <a href="https://creativecommons.org/licenses/by-sa/2.0/deed.en_GB" target="_blank" rel="noopener">Creative Commons Licence</A>.
</span></p>


<a name="from"></A><h2>From nets to the Internet</h2>

<p>Standardized PCs running standardized software brought a big benefit
for businesses: computers could be linked together into <a href="howcomputernetworkswork.html">networks</a> to
share information. At Xerox PARC in 1973, electrical engineer <span
 class="bold">Bob Metcalfe</span> (1946&ndash;) developed a new way of
linking computers "through the ether" (empty space) that he called
Ethernet. A few years later, Metcalfe left Xerox to form his own
company, 3Com, to help companies realize "Metcalfe's Law": computers
become useful the more closely connected they are to other people's
computers. As more and more companies explored the power of local area
networks (LANs), so, as the 1980s progressed, it became clear that
there were great benefits to be gained by connecting computers over
even greater distances&#8212;into so-called wide area networks (WANs). </p>

<P><img src="https://cdn4.explainthatstuff.com/ipod-touchscreen.jpg" class="ileftno" width=225 height=300 alt="An iPod Touch touchscreen."></P>
<p><span class="credit">Photo: Computers aren't what they used to be: they're much less noticeable because they're much more seamlessly integrated into everyday life. Some are "embedded" into household gadgets like coffee makers or <a href="television.html">televisions</A>. Others travel round in our pockets in our smartphones&mdash;essentially pocket computers that we can program simply by downloading "apps" (applications).</span></p>

<p>Today, the best known WAN is the <a href="internet.html">Internet</a>&#8212;a
global network of individual computers and LANs that links up hundreds
of millions of people. The history of the Internet is another story,
but it began in the 1960s when four American universities launched a
project to connect their computer systems together to make the first
WAN. Later, with funding for the Department of Defense, that network
became a bigger project called ARPANET (Advanced Research Projects
Agency Network). In the mid-1980s, the US National Science Foundation
(NSF) launched its own WAN called NSFNET. The convergence of all these
networks produced what we now call the Internet later in the 1980s.
Shortly afterward, the power of networking gave British computer
programmer <span class="bold">Tim Berners-Lee</span> (1955&ndash;) his big
idea: to combine the power of computer networks with the
information-sharing idea Vannevar Bush had proposed in 1945. Thus, was
born the <a href="howthewebworks.html">World Wide Web</a>&#8212;an easy way
of sharing information over a computer network, which made possible
the modern age of <a href="cloud-computing-introduction.html">cloud computing</A> 
(where anyone can access vast computing power over the Internet without having to worry
about where or how their data is processed). 
It's Tim Berners-Lee's invention that brings you this potted history of computing today!</p>

<div class="sharebot"><ul class="nobull">
<li class="shr"><iframe src="https://www.facebook.com/plugins/like.php?app_id=214994648542122&amp;href=http%3A%2F%2Fwww.explainthatstuff.com%2Fhistoryofcomputers.html&amp;send=false&amp;layout=button_count&amp;width=90&amp;show_faces=false&amp;action=like&amp;colorscheme=light&amp;font&amp;height=21" scrolling="no" frameborder="0" style="border:none; overflow:hidden; width:90px; height:21px;" allowTransparency="true"></iframe></li>
<!-- plusone and tweet -->
<!-- sharebuttons -->
<LI class="shr"><a target="_blank" rel="noopener" href="https://www.twitter.com/home?status=The+history+of+computers+https://www.explainthatstuff.com/historyofcomputers.html" title="Tweet this article"><IMG class="tweet" ALT="Tweet" src="https://cdn4.explainthatstuff.com/dot.gif"></a></LI>
</ul></div>


</div>

<!-- Desktop bottom ad -->
<div class="bottomsquare"><span class="slinks2">Sponsored links</span>
<P>
<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- etsbottomsquare -->
<ins class="adsbygoogle"
     style="display:inline-block;width:336px;height:280px"
     data-ad-client="ca-pub-1030585152417294"
     data-ad-slot="9721739364"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
</P>
</div>

<div class="main2">

<a name="fomo"></A><h2>Find out more</h2>


<h3>On this site</h3>
<UL>
<LI><a href="howcomputerswork.html">How computers work</A>: A basic guide.</LI>
<LI><a href="cloud-computing-introduction.html">Cloud computing</A>: How the Internet can provide access to vast, distributed computing power.</LI>
<LI><a href="buyingacomputer.html">Buying a new computer</A>: Laptop or desktop? New or secondhand? Windows, Max, or Linux? We list the most important considerations for computer buyers.</LI>
<LI><a href="quantum-computing.html">Quantum computing</a>: Will atomic-scale computers overcome the limitations of today's machines?</LI>
<li><a href="how-supercomputers-work.html">Supercomputers</a>: How do the world's most powerful computers work?</li>
<LI><a href="articles_computers.html">Computing articles</A>: A list of all the computer-related articles on this site.</LI>
</UL>

<h3>Other websites</h3>
<p>There are lots of websites covering computer history. Here are a just a few favorites worth exploring!</p>
<ul>
<li><a href="http://www.computerhistory.org/" target="_blank" rel="noopener">The Computer History Museum</a>: The website of the world's biggest
computer museum in California.</li>
<li><a href="http://news.bbc.co.uk/1/hi/in_depth/sci_tech/2007/the_computing_age/default.stm" target="_blank" rel="noopener">The Computing Age</a>: A BBC special report into computing past, present, and future.</li>
<li><a href="https://web.archive.org/web/20150702235036/http://www.sciencemuseum.org.uk/onlinestuff/stories/babbage.aspx" target="_blank" rel="noopener">Charles Babbage at the London Science Museum</a>: Lots of information about Babbage and his extraordinary engines. [Archived via the Wayback Machine]</li>
<li><a href="https://www.ibm.com/ibm/history/" target="_blank" rel="noopener">IBM History</a>: Many fascinating online exhibits, as well as inside
information about the part IBM inventors have played in wider computer history.</li>
<li><a href="https://en.wikipedia.org/wiki/History_of_computing_hardware" target="_blank" rel="noopener">Wikipedia History of Computing Hardware</a>: covers similar ground to this page.</li>
<li><a href="http://www.crowl.org/lawrence/history/" target="_blank" rel="noopener">Computer history images</a>: A small but interesting selection of photos.</li>
<li><a href="http://www.pbs.org/transistor/" target="_blank" rel="noopener">Transistorized!</a>: The history of the invention of the transistor from PBS.</li>
<li><a href="https://www.intel.com/content/www/us/en/company-overview/intel-museum.html" target="_blank" rel="noopener">Intel Museum</a>: The story of Intel's contributions to computing from the 1970s onward.</li>
</ul>

<h3>Videos</h3>
<p>There are some superb computer history videos on YouTube and elsewhere; here are three good ones to start you off:</p>
<ul>
<li><a href="https://www.youtube.com/watch?v=0anIyVGeWOI" target="_blank" rel="noopener">The Difference Engine</a>: A great introduction to Babbage's Difference Engine from Doron Swade, one of the world's leading Babbage experts.</li>
<li><a href="https://www.youtube.com/watch?v=OSYpYFEwr4o" target="_blank" rel="noopener">The ENIAC</a>: A short Movietone news clip about the completion of the world's first programmable electronic computer.</li>
<li><a href="https://www.youtube.com/watch?v=hXAjVw-bP5g" target="_blank" rel="noopener">A tour of the Computer History Museum</a>: Dag Spicer gives us a tour of the world's most famous computer museum, in California.</li>
</ul>

<h3>Books</h3>

<h4>For older readers</h4>
<UL>
<LI>Berners-Lee, Tim with Mark Fischetti. <a href="https://books.google.com/books?id=Unp4PwAACAAJ" target="_blank" rel="noopener">Weaving the Web: The
Original Design and Ultimate Destiny of the World Wide Web by its
Inventor. </A>San Fransisco, California: HarperCollins, 1999. Tim
Berners-Lee tells his own story of what he hopes to achieve with the
World Wide Web.</LI>
<LI>Cringely, Robert. <a href="https://books.google.com/books?id=eWUwIwAACAAJ" target="_blank" rel="noopener">Accidental Empires : How the Boys of Silicon
Valley Make Their Millions, Battle Foreign Competition, and Still Can't
Get a Date</A>. New York: HarperBusiness, 1996. The story of personal
computing in the 1980s.</LI>
<LI>Hodges, Andrew. <a href="https://books.google.com/books?id=VWvPIWm75XIC" target="_blank" rel="noopener">Alan Turing: The Enigma</A>. New York: Random House, 1992. Sets out Turing's fundamental contributions to the theory of computers and artificial intelligence and his pivotal importance as a wartime codebreaker, as well as telling his fascinating and tragic personal story.</LI>
<LI>Lean, Tom. <a href="https://books.google.com/books?id=RNaHCgAAQBAJ" target="_blank" rel="noopener">Electronic Dreams</A>. London: Bloomsbury Sigma, 2016. A fascinating and very readable history of the PC, told from a British perspective, and with an emphasis on British computer developers such as Sinclair, Acorn, and the BBC.</LI>
<LI>Levy, Steven. <a href="https://books.google.com/books?id=mShXzzKtpmEC" target="_blank" rel="noopener">Hackers: Heroes of the Computer Revolution</A>. New York: Penguin, 2010. The stories of creative computer pioneers such as Steve Wozniak.</LI>
<LI>Moschovitis, Christos, Hilary Poole, Laura Lambert, and Chris
Woodford. <a href="https://books.google.com/books?id=qi-ItIG6QLwC" target="_blank" rel="noopener">The Internet: A Historical Encyclopedia</A>. Santa
Barbara, California: ABC-Clio: 2005. A definitive three-volume history
of the Internet, including a chronology, a book of biographies, and a
look at the key issues challenging the Internet's development. Ask your
library to order it if you can't find it.</LI>
<LI>Riordan, Michael and Lillian Hoddeson. <a href="https://books.google.com/books?id=naPSt7ZkQhcC" target="_blank" rel="noopener">Crystal Fire: The
Invention of the Transistor and the Birth of the Information Age</A>.
New York: W. W. Norton &amp; Co., 1998. How John Bardeen, Walter
Brattain, and William Shockley invented the transistor&#8212;and how Robert
Noyce and Jack Kilby developed it thereafter.</LI>
<LI>Rojas, R&aacute;ul (ed). <a href="https://books.google.com/books?id=t4NUAAAAMAAJ" target="_blank" rel="noopener">Encyclopedia of Computers and Computer
History</A>. Chicago: Fitzroy Dearborn, 2001. A definitive two-volume
history. Ask your library to order it if you can't find it.</LI>
<LI>Swade, Doron. <a href="https://books.google.com/books?id=IWwGAAAACAAJ" target="_blank" rel="noopener">The Difference Engine: Charles Babbage and the
Quest to Build the First Computer</A>. New York: Viking, 2001. How
Babbage tried to build the first mechanical computer.</LI>
</UL>


<h4>For younger readers</h4>
<UL>
<LI>Gifford, Clive. <a href="https://books.google.com/books?id=Dtr2sgEACAAJ" target="_blank" rel="noopener">The Science of Computers</A>. London: Hachette, 2016. A solid, basic, 32-page introduction for ages 8&ndash;10.</LI>
<LI>Oxlade, Chris. <a href="https://books.google.com/books?id=1B_aDgAAQBAJ" target="_blank" rel="noopener">The History of Computers</A>. London: Raintree, 2018. This book covers roughly the same sequence of events as my article, but in simplified, 32-page form for ages 8&ndash;10.</LI>
<LI>Woodford, Chris. <a href="https://books.google.com/books?id=9RJXPQAACAAJ" target="_blank" rel="noopener">Communication and Computers</A>. New York: Facts on File, 2004. One of my own books, this puts the history of computers into a broader context, as part of the story of human communication technology. This is suitable for ages 10+.</LI>
</UL>


<H2>Notes and references</H2>

<OL>
<LI><a href="#1">&uarr;</a><a name="1fn"></a>&nbsp;&nbsp;&nbsp;There is a charming tribute to Boole in
<a href="https://bit.ly/2uRfU5l" target="_blank" rel="noopener">Popular Science</A>, October 1880, p.840.</LI>	
<LI><a href="#2">&uarr;</a><a name="2fn"></a>&nbsp;&nbsp;&nbsp;Babbage scholars tend to bristle at hagiography of Lovelace. 
For example, Doron Swade, who led the project to reconstruct Babbage's engines at the London Science Museum,
writes: "The notion that [Lovelace] made inspirational contribution to the development of the Engines is not supported by the known chronology of events... Posthumous eulogies of this kind, well intentioned as they are, have mythologised both her mathematical abilities and her role in the development of the Engines..." See <a href="https://books.google.com/books?id=XcQgHAAACAAJ" target="_blank" rel="noopener">The Cogwheel Brain</A>, Chapter 8.</LI>
<LI><a href="#3">&uarr;</a><a name="3fn"></a>&nbsp;&nbsp;&nbsp;<a href="https://www.theatlantic.com/magazine/archive/1945/07/as-we-may-think/303881/" target="_blank" rel="noopener">As We May Think</A> by Vannevar Bush, The Atlantic, July 1945.</LI>	
<LI><a href="#4">&uarr;</a><a name="4fn"></a>&nbsp;&nbsp;&nbsp;Zuse is perhaps one of the most unfairly neglected figures
in the history of computing. For a good account of his contributions, see [PDF]
<a href="http://page.mi.fu-berlin.de/rojas/1996/Konrad_Zuses_Legacy.pdf" target="_blank" rel="noopener">Konrad Zuse's Legacy: The Architecture of the Z1 and Z3</A> by R&aacute;ul Rojas, IEEE Annals of the History of Computing, Volume 19 Number 2, 1997, p.5</LI>	
<LI><a href="#5">&uarr;</a><a name="5fn"></a>&nbsp;&nbsp;&nbsp;The "father of radio" is about as accurate and useful a description as "the inventor of the computer": it could equally well be applied to Maxwell, Hertz, Lodge, Marconi, and (some claim) Tesla.</LI>	
<LI><a href="#6">&uarr;</a><a name="6fn"></a>&nbsp;&nbsp;&nbsp;Outside the computing world, von Neumann's contributions&mdash;summarized in the glib phrase "the stored program technique"&mdash;remain little known and even less understood.
For a very quick summary, see <a href="http://www.computinghistory.org.uk/det/3665/John-von-Neumann/" target="_blank" rel="noopener">John von Neumann</A> at the Centre
for Computing History; for much more detail, try <a href="https://books.google.com/books?id=c5uDQgAACAAJ" target="_blank" rel="noopener">John von Neumann and the Origins of Modern Computing</A> by William Aspray, MIT Press, 1990.</LI>	
<LI><a href="#7">&uarr;</a><a name="7fn"></a>&nbsp;&nbsp;&nbsp;You can listen to Maurice Wilkes describing
his life in an oral history recording from the
<a href="https://sounds.bl.uk/Oral-history/Science/021M-C1379X0021XX-0000V0" target="_blank" rel="noopener">British Library</A> made by Tom Lean in 2010.</LI>
<LI><a href="#8">&uarr;</a><a name="8fn"></a>&nbsp;&nbsp;&nbsp;The best account of this story is
<a href="https://books.google.com/books?id=naPSt7ZkQhcC" target="_blank" rel="noopener">Crystal Fire: The
Invention of the Transistor and the Birth of the Information Age</A> by Michael Riordan and Lillian Hoddeson.</LI>		
<LI><a href="#9">&uarr;</a><a name="9fn"></a>&nbsp;&nbsp;&nbsp;Steven Levy tells the early Apple story very well, positioning it in the wider context of computer history in <a href="https://books.google.com/books?id=mShXzzKtpmEC" target="_blank" rel="noopener">Hackers: Heroes of the Computer Revolution</A>.</LI>	
<LI><a href="#10">&uarr;</a><a name="10fn"></a>&nbsp;&nbsp;&nbsp;For a riveting account of the early PC years,
it's hard to beat <a href="https://books.google.com/books?id=eWUwIwAACAAJ" target="_blank" rel="noopener">Accidental Empires : How the Boys of Silicon
Valley Make Their Millions, Battle Foreign Competition, and Still Can't
Get a Date</A> by Robert X. Cringely. Tom Lean narrates the British side of the same story (Sinclair, Acorn, BBC Micro,
and so on) very well in his book <a href="https://books.google.com/books?id=RNaHCgAAQBAJ" target="_blank" rel="noopener">Electronic Dreams</A>.</LI>
</OL>

</div>



<!-- BTF ad  -->





<div class="cc"><P><span class="highlight">Please do NOT copy our articles onto blogs and other websites</span></P>
<p>Text copyright &copy; Chris Woodford 2006, 2018. All rights reserved. <a href="copyright-and-legal-notices.html">Full copyright notice and terms of use</A>.</p>


</div>


<div class="sharesites"><h2>Follow us</h2>
<ul class="nobull">
<LI class="inline2"><a href="followus.html" title="Follow us on Facebook"><IMG class="sprite shr4" ALT="Follow us on Facebook" src="https://cdn4.explainthatstuff.com/dot.gif"></A></li>
<!-- old google plus button link -->
<LI class="inline2"><a href="rss.xml" target="_blank" rel="noopener" title="Subscribe to our RSS feed"><IMG class="sprite shr10" ALT="RSS"  src="https://cdn4.explainthatstuff.com/dot.gif"></A></li>
<LI class="inline2"><a href="http://www.flickr.com/photos/explainthatstuff" target="_blank" rel="noopener" title="Find our photos on Flickr"><IMG class="sprite shr11" ALT="Flickr"  src="https://cdn4.explainthatstuff.com/dot.gif"></A></li>
<!-- old ptr social  -->
</ul>
</div>

<div class="sharesites"><h2>Rate this page</h2>
<p>Please <a href="feedback.php?1ex_field1=History+of+computers">rate or give feedback on this page</A> and I will make a donation to WaterAid.</P></div>



<div class="sharesites">
<h2>Share this page</h2> 
<p>Press CTRL + D to bookmark this page for later or tell your friends about it with:</p>

<ul class="nobull">
<LI class="inline2"><a title="Share this on Facebook" href="https://www.facebook.com/sharer.php?u=http://www.explainthatstuff.com/historyofcomputers.html"><IMG class="sprite shr4" ALT="Facebook" src="https://cdn4.explainthatstuff.com/dot.gif"></a></LI> 
<LI class="inline2">
<a title="Post this story to Delicious" href="http://del.icio.us/post?url=https://www.explainthatstuff.com/historyofcomputers.html&amp;title=History%20of%20computers"><IMG class="sprite shr1" ALT="Delicious" src="https://cdn4.explainthatstuff.com/dot.gif"></a></LI>
<LI class="inline2"><a title="Share this on Digg" href="http://digg.com/submit?url=https://www.explainthatstuff.com/historyofcomputers.html&amp;title=History%20of%20computers"><IMG class="sprite shr2" ALT="Digg" src="https://cdn4.explainthatstuff.com/dot.gif"></a></LI>
<LI class="inline2"><a title="Share this on Reddit" href="http://reddit.com/submit?url=https://www.explainthatstuff.com/historyofcomputers.html&amp;title=History%20of%20computers"><IMG class="sprite shr3" ALT="Reddit" src="https://cdn4.explainthatstuff.com/dot.gif"></a></LI>
<LI class="inline2"><a title="Share this on StumbleUpon" href="http://www.stumbleupon.com/submit?url=https://www.explainthatstuff.com/historyofcomputers.html&amp;title=History%20of%20computers"><IMG class="sprite shr5" ALT="StumbleUpon" src="https://cdn4.explainthatstuff.com/dot.gif"></a></LI>
<LI class="inline2"><a title="Add this to your Google Bookmarks"
 href="http://www.google.com/bookmarks/mark?op=edit&amp;bkmk=https://www.explainthatstuff.com/historyofcomputers.html&amp;title=History%20of%20computers"><IMG class="sprite shr6" ALT="Google Bookmarks" src="https://cdn4.explainthatstuff.com/dot.gif"></a></LI>
<LI class="inline2"><a href="https://www.twitter.com/home?status=The+history+of+computers+https://www.explainthatstuff.com/historyofcomputers.html" title="Tweet this"><IMG class="sprite shr7" ALT="Twitter" src="https://cdn4.explainthatstuff.com/dot.gif"></a></LI>
<LI class="inline2"><a title="Email this page to a friend"
 href="mailto:?subject=History%20of%20computers&amp;body=Take%20a%20look%20at%20this%20page:%20https://www.explainthatstuff.com/historyofcomputers.html"><IMG class="sprite shr8" ALT="Email" src="https://cdn4.explainthatstuff.com/dot.gif"></a></LI>
<!-- old rel = author link -->
</UL>


<h2>Cite this page</h2>

<!---Citation-->
<div class="citation">
<P>
Woodford, Chris. (2006/2018) History of Computers. Retrieved from https://www.explainthatstuff.com/historyofcomputers.html. [Accessed (Insert date here)]
</P>
</div>

</div>



<!-- advinfo2 -->

<!-- bottom search position -->








<div class="alsoon"><a name="navigation"></a>
<IMG class="footlogo" ALT="Explain that Stuff" src="https://cdn4.explainthatstuff.com/dot.gif">
<h2>More to explore on our website...</h2>
<ul class="nobull">
<LI class="inline"><a href="articles_communications.html">Communications</A></LI>
<LI class="inline"><a href="articles_computers.html">Computers</A></LI>
<LI class="inline"><a href="articles_electricity.html">Electricity &amp; electronics</A></LI>
<LI class="inline"><a href="articles_energy.html">Energy</A></LI>
<LI class="inline"><a href="articles_engineering.html">Engineering</A></LI>
<LI class="inline2"><a href="articles_environment.html">Environment</A></LI><BR>
<LI class="inline"><a href="articles_gadgets.html">Gadgets</A></LI>
<LI class="inline"><a href="articles_homelife.html">Home life</A></LI>
<LI class="inline"><a href="articles_materials.html">Materials</A></LI>
<LI class="inline"><a href="articles_science.html">Science</A></LI>
<LI class="inline"><a href="articles_tools.html">Tools &amp; instruments</A></LI>
<LI class="inline2"><a href="articles_transportation.html">Transportation</A></LI>
</UL>
</div>

<div class="footer">
<ul class="nobull"><li class="inline"><a href="/">Home</a></li>
<li class="inline"><a href="azindex.html">A-Z index</a></li>
<li class="inline"><a href="mybooks.php">Get the book</a></li>
<li class="inline"><a href="followus.html">Follow us</a></li>
<li class="inline"><a href="random.php">Random article</a></li>
<li class="inline"><a href="timeline.html">Timeline</a></li>
<li class="inline"><a href="teaching-guide.html">Teaching guide</a></li>
<li class="inline"><a href="aboutus.html">About us</a></li>
<li class="inline2"><a href="privacy.html">Privacy &amp; cookies</a></li></ul>
</div>

<div class="totop">&uarr; <a href="#pagetop">Back to top</a></div>
</div>


<!-- old mobile search and plus one -->

</body>
</html>
